{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ad59cde-3406-466a-9682-d692224a612c",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://laelgelcpublic.s3.sa-east-1.amazonaws.com/lael_50_years_narrow_white.png.no_years.400px_96dpi.png\" width=\"300\" alt=\"LAEL 50 years logo\">\n",
    "<h3>APPLIED LINGUISTICS GRADUATE PROGRAMME (LAEL)</h3>\n",
    "</center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5577bc20-e9f8-48f4-a715-081ad8d67c6a",
   "metadata": {},
   "source": [
    "# Programme to uncompress the archives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756a91c9-31a8-4ac8-a6d3-8cdddd2baf26",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec25b133-4f16-4c15-b8d2-e040ec11e504",
   "metadata": {},
   "source": [
    "### Environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d184c1-c7d2-499c-a51e-b25b6216ab53",
   "metadata": {},
   "source": [
    "AWS credentials and other parameters should be stored in the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea79a666-dc7f-465d-a8ab-282918726eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_ACCESS_KEY_ID=<YOUR_ACCESS_KEY_ID>\n",
    "AWS_SECRET_ACCESS_KEY=<YOUR_SECRET_ACCESS_KEY>\n",
    "REGION_NAME=<YOUR_REGION_NAME>\n",
    "SOURCE_BUCKET_NAME=<YOUR_SOURCE_BUCKET_NAME>\n",
    "DESTINATION_BUCKET_NAME=<YOUR_DESTINATION_BUCKET_NAME>\n",
    "INPUT_DIRECTORY=<OUTPUT_DIRECTORY_input>\n",
    "OUTPUT_DIRECTORY=<OUTPUT_DIRECTORY>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0483e25e-7c2e-4266-8cfc-21b0d76d22b7",
   "metadata": {},
   "source": [
    "### Required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4de9ff-d5d5-4d32-b2ec-6dfaeea74cd7",
   "metadata": {},
   "source": [
    "The required libraries are going to be stored in the `unarchive.req` file.\n",
    "- Create the environment `my_env` with the command: `python3 -m venv my_env`\n",
    "- Activate the `my_env`: `cd my_env && source bin/activate`\n",
    "- The following command should be executed: `pip install -r unarchive.req`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfe3b39-a2fb-4f03-b22a-f9d06c073ff6",
   "metadata": {},
   "source": [
    "#### Contents of `unarchive.req`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87bfd18-8a7c-45ce-b488-d3682fe3b1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "python-dotenv\n",
    "boto3\n",
    "pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831f3c64-b560-403d-8013-5d83534eb0a0",
   "metadata": {},
   "source": [
    "### Execution in the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a30585-b0b7-4895-951a-7dafd1dbe9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "python unarchive.py > unarchive.log 2>&1 &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d1357d-0deb-4c96-89e0-8b3358579c19",
   "metadata": {},
   "source": [
    "- `python unarchive.py` runs the Python programme\n",
    "- `> unarchive.log` redirects the standard output to the `log.txt` file\n",
    "- `2>&1` redirects the standard error to the same file\n",
    "- `&` runs the command in the background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e0190d-cac2-4990-a5c1-5cf70d1dea1e",
   "metadata": {},
   "source": [
    "## Code of `unarchive.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acca8d4-9a42-493e-9171-ca397ac9d8b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Edit the file '.env' and provide the required parameters\n",
    "# Install the required libraries in the environment by executing: 'pip install -r unarchive.req'\n",
    "\n",
    "# Importing the required libraries\n",
    "from dotenv import load_dotenv\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import bz2\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "load_dotenv()  # This line brings all environment variables from '.env' into 'os.environ'\n",
    "\n",
    "# Define the name of the CSV file containing the list of S3 keys\n",
    "key_list = 'unarchive_key_list_test.csv'\n",
    "#key_list = 'unarchive_key_list_2011.csv'\n",
    "#key_list = 'unarchive_key_list_2012.csv'\n",
    "#key_list = 'unarchive_key_list_2013.csv'\n",
    "#key_list = 'unarchive_key_list_2014.csv'\n",
    "#key_list = 'unarchive_key_list_2015.csv'\n",
    "#key_list = 'unarchive_key_list_2016.csv'\n",
    "#key_list = 'unarchive_key_list_2017.csv'\n",
    "#key_list = 'unarchive_key_list_2018.csv'\n",
    "#key_list = 'unarchive_key_list_2019.csv'\n",
    "#key_list = 'unarchive_key_list_2020.csv'\n",
    "#key_list = 'unarchive_key_list_2021.csv'\n",
    "#key_list = 'unarchive_key_list_2022.csv'\n",
    "#key_list = 'unarchive_key_list_2023.csv'\n",
    "\n",
    "# Set up AWS credentials\n",
    "aws_access_key_id = os.environ['AWS_ACCESS_KEY_ID']\n",
    "aws_secret_access_key = os.environ['AWS_SECRET_ACCESS_KEY']\n",
    "region_name = os.environ['REGION_NAME']\n",
    "\n",
    "# Set up S3 client\n",
    "s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, region_name=region_name)\n",
    "\n",
    "# Set up the source and destination S3 bucket names\n",
    "source_bucket_name = os.environ['SOURCE_BUCKET_NAME']\n",
    "destination_bucket_name = os.environ['DESTINATION_BUCKET_NAME']\n",
    "\n",
    "# Define the name of the directory where the downloaded files will be stored\n",
    "input_directory = os.environ['INPUT_DIRECTORY']\n",
    "\n",
    "# Check if the input directory already exists. If it does, remove it and its contents. If it doesn't exist, create it.\n",
    "if os.path.exists(input_directory):\n",
    "    shutil.rmtree(input_directory)\n",
    "    print('Old output directory successfully removed.')\n",
    "    try:\n",
    "        os.makedirs(input_directory)\n",
    "        print('Output directory successfully created.')\n",
    "    except OSError as e:\n",
    "        print('Failed to create the directory:', e)\n",
    "        sys.exit(1)\n",
    "else:\n",
    "    try:\n",
    "        os.makedirs(input_directory)\n",
    "        print('Output directory successfully created.')\n",
    "    except OSError as e:\n",
    "        print('Failed to create the directory:', e)\n",
    "        sys.exit(1)\n",
    "\n",
    "# Define the name of the directory where the unarchived files will be stored\n",
    "output_directory = os.environ['OUTPUT_DIRECTORY']\n",
    "\n",
    "# Check if the output directory already exists. If it does, remove it and its contents. If it doesn't exist, create it.\n",
    "if os.path.exists(output_directory):\n",
    "    shutil.rmtree(output_directory)\n",
    "    print('Old output directory successfully removed.')\n",
    "    try:\n",
    "        os.makedirs(output_directory)\n",
    "        print('Output directory successfully created.')\n",
    "    except OSError as e:\n",
    "        print('Failed to create the directory:', e)\n",
    "        sys.exit(1)\n",
    "else:\n",
    "    try:\n",
    "        os.makedirs(output_directory)\n",
    "        print('Output directory successfully created.')\n",
    "    except OSError as e:\n",
    "        print('Failed to create the directory:', e)\n",
    "        sys.exit(1)\n",
    "\n",
    "# Read the key CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(key_list, header=0)\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    tar_file_key = row['filename-destination']\n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(timestamp, ': Downloading ' + tar_file_key)\n",
    "    s3.download_file(source_bucket_name, tar_file_key, input_directory + '/' + tar_file_key)\n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(timestamp, ': Extracting ' + tar_file_key)\n",
    "    with tarfile.open(input_directory + '/' + tar_file_key, 'r') as tar:\n",
    "        tar.extractall(path=output_directory)\n",
    "    # Iterate over the extracted files\n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(timestamp, ': Extracting and transferring .bz2 files to S3 for ' + tar_file_key)\n",
    "    for root, dirs, files in os.walk(output_directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.bz2'):\n",
    "                # Uncompress each .bz2 file\n",
    "                with bz2.open(os.path.join(root, file), 'rb') as bz_file:\n",
    "                    uncompressed_data = bz_file.read()\n",
    "                \n",
    "                    # Get the relative path of the file within the directory tree\n",
    "                    relative_path = os.path.relpath(os.path.join(root, file), '.')\n",
    "                \n",
    "                    # Upload the processed file to the destination S3 bucket with the same directory tree structure\n",
    "                    destination_key = os.path.join(relative_path, file)\n",
    "                    s3.put_object(Body=uncompressed_data, Bucket=destination_bucket_name, Key=destination_key)\n",
    "    shutil.rmtree(output_directory)\n",
    "    os.makedirs(output_directory)\n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(timestamp, ': Output directory cleared out for ' + tar_file_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092c38fc-7d72-4db9-9aca-13e70dfd386e",
   "metadata": {},
   "source": [
    "## Sample code to parse the tweets files ([JSONL](https://spark.apache.org/docs/latest/sql-data-sources-json.html) format) to a dataframe over Amazon EMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25fb2d0-2f9a-4ab4-acf2-64d450488311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Tweet Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the JSONL files from S3 and create a dataframe\n",
    "tweets_df = spark.read.json(\"s3://your-bucket/tweets.jsonl\", multiLine=True)\n",
    "\n",
    "# Filter the dataframe to extract the desired data\n",
    "filtered_df = tweets_df.filter(tweets_df['text'].contains(\"your-filter-keyword\"))\n",
    "\n",
    "# Show the filtered dataframe\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7e9b00-8d6d-41ed-92c0-c220e89e5e22",
   "metadata": {},
   "source": [
    "# snscrape format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b79b5e-dc6f-4517-98dd-6f9ad1ebb9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the JSON file into a DataFrame\n",
    "df1 = pd.read_json('snscrape.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41c4c74-1211-4db8-bfb1-3b287039b6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a47fdc-320c-4538-bd2a-37e9fdaba05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "quoted_tweet = df1.loc[7, 'quotedTweet']\n",
    "quoted_tweet_str = json.dumps(quoted_tweet)\n",
    "quoted_tweet_dict = json.loads(quoted_tweet_str)\n",
    "\n",
    "media = quoted_tweet_dict['media']\n",
    "print(media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb093f2-2577-40c0-ab85-f3996c39e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f82af70-d78c-4b1b-91d6-7bed63b9fbea",
   "metadata": {},
   "source": [
    "# Internet Archive format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4a009d-47b5-4a12-a7ac-7df697d8ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the JSON file into a DataFrame\n",
    "df2 = pd.read_json('intarch.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02cb651-a933-47b8-a913-c39b25f933db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cbb417-633f-4a6e-aed4-5a278724f364",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbf0963-495e-44da-9114-cc7b312e4206",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
